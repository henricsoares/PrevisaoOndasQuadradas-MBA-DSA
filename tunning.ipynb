{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import json\n",
    "import optuna\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.metrics import r2_score\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "from _utils import NeuralNetwork, test_model_with_new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = {}\n",
    "with open(\"./dataset.json\", \"r\") as arquivo:\n",
    "    dados = json.load(arquivo)\n",
    "\n",
    "data = [(d[\"coords\"]) for d in dados[\"dados\"]]\n",
    "target = [d[\"params\"] for d in dados[\"dados\"]]\n",
    "\n",
    "# Converta 'data' para um array bidimensional\n",
    "data_flattened = [sample for series in data for sample in series]\n",
    "\n",
    "# Crie uma instância do MinMaxScaler e ajuste aos dados\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(data_flattened)\n",
    "\n",
    "# Normalize os dados\n",
    "data_normalized = [scaler.transform(series) for series in data]\n",
    "\n",
    "scaler.fit(target)\n",
    "\n",
    "# Normalize os rótulos\n",
    "target_normalized = scaler.transform(target)\n",
    "\n",
    "# Carregue seus dados\n",
    "test_size = 0.2  # 20% dos dados para teste\n",
    "train_data, test_data, train_target, test_target = train_test_split(\n",
    "    data_normalized, target_normalized, test_size=test_size, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, metric=\"mse\"):\n",
    "    \"\"\"Avalia um dado modelo com base em um test_loader e uma métrica de avaliação. 'r2' para r², 'pond' para a ponderada do r² e mse, e qualquer outro valor para mse.\"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            outputs = model(batch_x)\n",
    "            test_loss += criterion(outputs, batch_y).item()\n",
    "\n",
    "            all_predictions.extend(outputs.cpu().numpy())\n",
    "            all_targets.extend(batch_y.cpu().numpy())\n",
    "\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "    r2 = r2_score(all_targets, all_predictions)\n",
    "    mse = test_loss / len(test_loader)\n",
    "\n",
    "    if metric == \"pond\":\n",
    "        return r2 - mse\n",
    "\n",
    "    if metric == \"r2\":\n",
    "        return r2\n",
    "    else:\n",
    "        return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    ####################### defina o espaço de hiperparâmetros\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 5, step=1)\n",
    "    units_fc = [\n",
    "        trial.suggest_int(f\"units_fc_layer_{i}\", 16, 256, step=16)\n",
    "        for i in range(num_layers)\n",
    "    ]\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 16, 128, step=16)\n",
    "    epochs = trial.suggest_int(\"epochs\", 10, 100, step=10)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True)\n",
    "    num_folds = trial.suggest_int(\"num_folds\", 2, 20, step=1)\n",
    "    #######################\n",
    "\n",
    "    model = NeuralNetwork(num_layers, units_fc)\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    train_dataset_size = len(train_data)\n",
    "    k_fold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (train_indices, val_indices) in enumerate(\n",
    "        k_fold.split(range(train_dataset_size))\n",
    "    ):\n",
    "        # Configure os conjuntos de dados e rótulos\n",
    "        x_train_set = np.array(Subset(train_data, train_indices))\n",
    "        y_train_set = np.array(Subset(train_target, train_indices))\n",
    "        x_val_set = np.array(Subset(train_data, val_indices))\n",
    "        y_val_set = np.array(Subset(train_target, val_indices))\n",
    "\n",
    "        x_train_tensor = torch.Tensor(x_train_set)\n",
    "        y_train_tensor = torch.Tensor(y_train_set)\n",
    "        x_val_tensor = torch.Tensor(x_val_set)\n",
    "        y_val_tensor = torch.Tensor(y_val_set)\n",
    "\n",
    "        train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Treine o modelo\n",
    "        for _ in range(int(epochs / num_folds)):\n",
    "            model.train()\n",
    "\n",
    "            # Loop de treinamento\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Avalie o modelo e retorne a métrica\n",
    "            loss = evaluate_model(model, val_loader, \"mse\")\n",
    "            fold_metrics.append(loss)\n",
    "    return np.mean(fold_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(\n",
    "    direction=\"minimize\"\n",
    ")  # Direction = \"maximize\" para r² e ponderada, \"minimize\" para mse\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(study, \"estudo.pkl\")  # salve seu modelo para uso futuro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confira o melhor resultado para seu estudo\n",
    "best_params = study.best_trial.params\n",
    "metrica = study.best_trial.values\n",
    "best_params, metrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize as tentativas do estudo\n",
    "import optuna.visualization as vis\n",
    "\n",
    "vis.plot_optimization_history(study)\n",
    "vis.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordena os 10 melhores resultados pela métrica (ordem descendente para r2, ascendente para mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = study.trials\n",
    "for t in trials:\n",
    "    t.params.update({\"value\": t.value})\n",
    "trials_params = [t.params for t in trials]\n",
    "trials_params_filtered = [\n",
    "    trial for trial in trials_params if trial[\"value\"] is not None\n",
    "]\n",
    "lista_ordenada = sorted(\n",
    "    trials_params_filtered, key=lambda x: x[\"value\"], reverse=True\n",
    ")  # reverse = true para r² e pond, false para mse\n",
    "lista_ordenada = lista_ordenada[:10]\n",
    "lista_ordenada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(params):\n",
    "    units_fc = [params[f\"units_fc_layer_{i}\"] for i in range(params[\"num_layers\"])]\n",
    "    num_layers = params[\"num_layers\"]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    epochs = params[\"epochs\"]\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    num_folds = params[\"num_folds\"]\n",
    "    model = NeuralNetwork(num_layers, units_fc)\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    train_dataset_size = len(train_data)\n",
    "    k_fold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    r2_train_values = []\n",
    "    r2_val_values = []\n",
    "\n",
    "    for fold, (train_indices, val_indices) in enumerate(\n",
    "        k_fold.split(range(train_dataset_size))\n",
    "    ):\n",
    "        # Configure os conjuntos de dados e rótulos\n",
    "        x_train_set = np.array(Subset(train_data, train_indices))\n",
    "        y_train_set = np.array(Subset(train_target, train_indices))\n",
    "        x_val_set = np.array(Subset(train_data, val_indices))\n",
    "        y_val_set = np.array(Subset(train_target, val_indices))\n",
    "\n",
    "        x_train_tensor = torch.Tensor(x_train_set)\n",
    "        y_train_tensor = torch.Tensor(y_train_set)\n",
    "        x_val_tensor = torch.Tensor(x_val_set)\n",
    "        y_val_tensor = torch.Tensor(y_val_set)\n",
    "\n",
    "        train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_val_loss = 0.0\n",
    "\n",
    "        # Treine e valide o modelo\n",
    "        for epoch in range(int(epochs / num_folds)):\n",
    "            model.train()\n",
    "            all_predictions = []\n",
    "            all_targets = []\n",
    "\n",
    "            # Loop de treinamento\n",
    "            for batch_x, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                all_predictions.extend(outputs.cpu().detach().numpy())\n",
    "                all_targets.extend(batch_y.cpu().detach().numpy())\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            all_predictions = np.array(all_predictions)\n",
    "            all_targets = np.array(all_targets)\n",
    "            r2 = r2_score(all_targets, all_predictions)\n",
    "            r2_train_values.append(r2)\n",
    "\n",
    "            # Loop de validação\n",
    "            model.eval()\n",
    "            all_predictions = []\n",
    "            all_targets = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    outputs = model(batch_x)\n",
    "                    val_loss = criterion(outputs, batch_y)\n",
    "                    running_val_loss += val_loss.item()\n",
    "                    all_predictions.extend(outputs.cpu().numpy())\n",
    "                    all_targets.extend(batch_y.cpu().numpy())\n",
    "                val_losses.append(val_loss.item())\n",
    "            all_predictions = np.array(all_predictions)\n",
    "            all_targets = np.array(all_targets)\n",
    "            r2 = r2_score(all_targets, all_predictions)\n",
    "            r2_val_values.append(r2)\n",
    "\n",
    "    average_train_loss = np.mean(train_losses)\n",
    "    average_val_loss = np.mean(val_losses)\n",
    "    average_train_r2 = np.mean(r2_train_values)\n",
    "    average_val_r2 = np.mean(r2_val_values)\n",
    "    print(\n",
    "        f\"average train_loss: {average_train_loss}, \\n\"\n",
    "        f\"average val_loss: {average_val_loss}, \\n\"\n",
    "        f\"average train R²: {average_train_r2}, \\n\"\n",
    "        f\"average val R²: {average_val_r2}\\n\"\n",
    "    )\n",
    "    return average_train_loss, average_val_loss, average_train_r2, average_val_r2, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carrega estudos feitos previamente para utilização ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o estudo de um arquivo .pkl usando joblib\n",
    "\n",
    "study_only_mse = joblib.load(\"estudo_mse_2103.pkl\")\n",
    "\n",
    "study_mse_r2 = joblib.load(\"estudo_pond_2103.pkl\")\n",
    "\n",
    "study_only_r2 = joblib.load(\"estudo_r2_2103.pkl\")\n",
    "\n",
    "best_trial_mse, best_trial_r2, best_trial_pond = (\n",
    "    study_only_mse.best_trial,\n",
    "    study_only_r2.best_trial,\n",
    "    study_mse_r2.best_trial,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial_mse = {key: value for key, value in best_trial_mse.params.items()}\n",
    "best_trial_r2 = {key: value for key, value in best_trial_r2.params.items()}\n",
    "best_trial_pond = {key: value for key, value in best_trial_pond.params.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_mse, best_model_r2, best_model_mser2 = (\n",
    "    train_and_validate(best_trial_mse),\n",
    "    train_and_validate(best_trial_r2),\n",
    "    train_and_validate(best_trial_pond),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salve o modelo\n",
    "path = \"modeloD1.pth\"\n",
    "torch.save(best_model_mse[4].state_dict(), path)\n",
    "\n",
    "path = \"modeloD2.pth\"\n",
    "torch.save(best_model_r2[4].state_dict(), path)\n",
    "\n",
    "path = \"modeloD3.pth\"\n",
    "torch.save(best_model_mser2[4].state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados = {}\n",
    "with open(\"./newData.json\", \"r\") as arquivo:\n",
    "    dados = json.load(arquivo)\n",
    "pontos = [(d[\"coords\"]) for d in dados[\"dados\"]][0]\n",
    "params = [d[\"params\"] for d in dados[\"dados\"]]\n",
    "\n",
    "scaler.fit(pontos)\n",
    "pontos_n = scaler.transform(pontos)\n",
    "pontos_tensor = torch.Tensor(pontos_n)\n",
    "\n",
    "previsao1 = best_model_mse[4](pontos_tensor.unsqueeze(0))\n",
    "previsao2 = best_model_r2[4](pontos_tensor.unsqueeze(0))\n",
    "previsao3 = best_model_mser2[4](pontos_tensor.unsqueeze(0))\n",
    "\n",
    "\n",
    "def calcula_mse(predictions, labels):\n",
    "    mse = ((predictions - labels) ** 2).mean()\n",
    "    return mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para gerar o gráfico\n",
    "def plot_grafico(linhas, new_data, ax, titulo):\n",
    "    # Defina o tamanho do gráfico\n",
    "    x_min, x_max = 0, 6000\n",
    "    y_min, y_max = 0, 8\n",
    "    dmin, dmax = linhas[2], linhas[3]\n",
    "    tl, th = linhas[0], linhas[1]\n",
    "\n",
    "    x_line = [[x_min, dmin, dmin, dmax, dmax, x_max]]\n",
    "\n",
    "    y_line = [[tl, tl, th, th, tl, tl]]\n",
    "\n",
    "    ax.plot(x_line[0], y_line[0], color=\"red\")\n",
    "\n",
    "    ax.scatter([x[0] for x in new_data], [y[1] for y in new_data], c=\"blue\")\n",
    "\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_title(titulo, loc=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(6, 18))\n",
    "\n",
    "# Converta a previsão para coordenadas no gráfico\n",
    "scaler.fit(target)\n",
    "previsao_c1 = scaler.inverse_transform(previsao1.detach().numpy())\n",
    "linhas1 = previsao_c1.tolist()[0]\n",
    "previsao_c2 = scaler.inverse_transform(previsao2.detach().numpy())\n",
    "linhas2 = previsao_c2.tolist()[0]\n",
    "previsao_c3 = scaler.inverse_transform(previsao3.detach().numpy())\n",
    "linhas3 = previsao_c3.tolist()[0]\n",
    "\n",
    "# Plote os gráficos\n",
    "plot_grafico(linhas1, pontos, axs[0], \"A.   Modelo D.4\")\n",
    "plot_grafico(linhas2, pontos, axs[1], \"B.   Modelo D.5\")\n",
    "plot_grafico(linhas3, pontos, axs[2], \"C.   Modelo D.6\")\n",
    "\n",
    "oParams = scaler.transform(params)\n",
    "pondParams = previsao3.detach().numpy()\n",
    "r2Params = previsao2.detach().numpy()\n",
    "mseParams = previsao1.detach().numpy()\n",
    "\n",
    "mse1 = calcula_mse(mseParams, oParams)\n",
    "mse2 = calcula_mse(r2Params, oParams)\n",
    "mse3 = calcula_mse(pondParams, oParams)\n",
    "\n",
    "axs[0].text(\n",
    "    0.5, 0.9, f\"MSE: {mse1:.5f}\", transform=axs[0].transAxes, ha=\"center\", fontsize=14\n",
    ")\n",
    "axs[1].text(\n",
    "    0.5, 0.9, f\"MSE: {mse2:.5f}\", transform=axs[1].transAxes, ha=\"center\", fontsize=14\n",
    ")\n",
    "axs[2].text(\n",
    "    0.5, 0.9, f\"MSE: {mse3:.5f}\", transform=axs[2].transAxes, ha=\"center\", fontsize=14\n",
    ")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=14)\n",
    "    ax.set_xlabel(ax.get_xlabel(), fontsize=14)\n",
    "    ax.set_ylabel(ax.get_ylabel(), fontsize=14)\n",
    "    ax.set_title(ax.get_title(), fontsize=14)\n",
    "\n",
    "plt.savefig(\"comparação_previsao_mse_r2_ponderada_new_space.png\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tensor = torch.Tensor(np.array(test_data))\n",
    "y_test_tensor = torch.Tensor(np.array(test_target))\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_model_with_new_data(best_model_mse[4], test_loader)\n",
    "test_model_with_new_data(best_model_r2[4], test_loader)\n",
    "test_model_with_new_data(best_model_mser2[4], test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
